---
title: "p8105_hw6_jt3387"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(pheatmap)
library(ggcorrplot)
library(leaps)
library(patchwork)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

## Problem 2

Load and clean the data set.

```{r}
raw_data <- read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
homicide <- raw_data %>% 
  janitor::clean_names() %>%
  mutate(city_state = str_c(city, state, sep = ", "),
         resolution = if_else(disposition == "Closed by arrest", 1, 0),
         victim_age = as.numeric(victim_age)) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
         victim_race %in% c("White", "Black"))
```

Fit a logistic regression model for city of Baltimore, MD.

```{r}
baltimore_df = homicide %>%
  filter(city_state == "Baltimore, MD")

baltimore_glm = 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial, data = baltimore_df) %>%
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         CI_lower = exp(estimate - 1.96 * std.error),
         CI_upper = exp(estimate + 1.96 * std.error)) %>%
  filter(term == "victim_sexMale") %>%
  select(term, OR, CI_lower, CI_upper)

baltimore_glm %>% 
  knitr::kable(digits = 3)
```

- The estimate value of the adjusted odds ratio is 0.426, the confidence interval is (0.325, 0.558).

Run glm for each of the cities.

```{r}
all_city_glm = homicide %>% 
  nest(data = -city_state) %>% 
  mutate(model = map(data, ~glm(resolution ~ victim_age + victim_sex + victim_race, 
                                family = binomial, data = .x)), 
         tidy_result = map(model, broom::tidy)) %>% 
  select(city_state, tidy_result) %>% 
  unnest(tidy_result) %>%
  mutate(OR = exp(estimate), 
         CI_lower = exp(estimate - 1.96 * std.error), 
         CI_upper = exp(estimate + 1.96 * std.error)) %>%
  filter(term == "victim_sexMale") %>%
  select(city_state, OR, CI_lower, CI_upper)
all_city_glm %>% 
  knitr::kable(digits = 3)
```

Create a plot that shows the estimated ORs and CIs for each city.

```{r}
all_city_glm %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  labs(
    title = "Estimated OR and CI for Each City",
    x = "City, State",
    y = "Estimated Odds Ratio") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Comments on the plot:

- Most of the cities has a estimated odds ratio below 1, which indicates homicides with male victim have lower chance to be resolved than those with female victim.

- New York, NY has the lowest estimated odds ratio and a relatively narrow CI, while Albuquerque, NM has the highest estimated odds ratio and a relatively broad CI among all cities.

- Some cities' CI contain 1, so there is not significant difference in chance of solving the homicides with male or female victims at 0.05 significant level. Also, some cities have fairly broad confidence intervals which indicates dispersion in the data, so the estimated result must be interpreted with caution.

## Problem 3

Load and clean the data set.

```{r message = FALSE}
birthweight <-read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(babysex = factor(recode(babysex, `1` = "Male", `2` = "Female")),
         frace = factor(recode(frace, `1` = "White", `2` = "Black", `3` = "Asian",
                        `4` = "Puerto Rican", `8` = "Other", `9` = "Unknown")),
         malform = factor(recode(malform, `0` = "Absent", `1` = "Present")),
         mrace = factor(recode(mrace, `1` = "White", `2` = "Black", `3` = "Asian",
                               `4` = "Puerto Rican", `8` = "Other"))
  )
skimr::skim(birthweight)
birthweight1 <- birthweight %>% 
  mutate(parity = factor(parity))
```

- I converted four variables: `babysex`, `frace`, `malform`, `mrace` into factors and recoded their values to the associated levels, then checked for missing data and the result indicated that there is no missing data.

Have a overview of the statistics of variables.

```{r}
summary(birthweight)
```

-  We can find that there are only 0 values in variables `pnumlbw` and `pnumsga`, for variable `parity`, only 3 values are not 0 while other 4339 values are all 0, and variable `malform` has the similar situation. Therefore they are not suitable to be predictors for regression. 

Check the distribution of the outcome to see whether it satisfies the normality assumption and whether it needs transformation.

```{r message = FALSE}
birthweight %>% ggplot(aes(x = bwt)) + 
  geom_histogram(aes(y = ..density..), fill = "light blue") +
  geom_density() + 
  labs(
    title = "Birthweight Distribution",
    x = "Birthweight (grams)",
    y = "Density"
  )
```

- From the distribution curve, we can see that the distribution of birthweight is roughly normal. Therefore, we think that it satisfies the normal assumption.

Draw a correlation coefficient matrix to see the relationship between different variables.

```{r}
cor <- birthweight %>% 
  select(2:6, 8, 10:12, 17:20) %>% 
  cor() %>% 
  round(3)
ggcorrplot(cor, type = "lower", hc.order = TRUE, lab = TRUE, lab_size = 3) + 
  guides(fill = guide_legend(title = "Pearson\nCorrelation"))
```

- From the correlation coefficient matrix, we can see that the correlation coefficients between some variables are very large such as `delwt` and `ppwt`, `ppbmi` and `ppwt`, so introducing all of them into the regression model will result in multicollinearity problems. 

Use boxplot to check the distribution of variables.

```{r}
pivotdata <- birthweight %>% 
  select(2, 3, 5, 6, 8, 10:12, 17:20) %>% 
  pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "value"
  )
pivotdata %>% 
  ggplot(aes(factor(variable), value)) + 
  geom_boxplot(
    aes(color = variable), 
    show.legend = FALSE, outlier.size = .8) + 
  facet_wrap(~variable, scale = "free") + 
  labs(
    title = "Boxplots of 12 numeric variables",
    x = "Variable",
    y = "Value"
  )
```

- From the boxplot, we can see that many variables' IQR is not large and there are also outliers in them.

Have a overview of the preliminary model based on previous analysis and known associations between birth weight and other variables, and find a better subset using `regsubsets` function.

```{r}
pre_model = lm(bwt ~ babysex + bhead + blength + frace + gaweeks + mheight + momage + mrace + smoken + wtgain, data = birthweight)
summary(pre_model) %>% 
  broom::tidy()
```

- From the regression result, we can see that, the coefficients of variables `frace`, `momage`, and `mrace` are not statistically significant, which all have p-values greater than 0.05.